%% BioMed_Central_Tex_Template_v1.06
\documentclass{bmcart}
%\documentclass[11pt]{article}
\usepackage[export]{adjustbox}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}  % Define \bm{} to use bold math fonts
\usepackage{csvsimple}
\usepackage{dcolumn}
\usepackage{flafter}  % Don't place floats before their definition
\usepackage{graphicx}
\usepackage{geometry}
\usepackage[utf8]{inputenc} %unicode support
\usepackage{lscape}
\usepackage{mathtools}
\usepackage{MnSymbol}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{url}


%\usepackage{fullpage}
\bibliographystyle{cbe}
\citestyle{aa}

%\usepackage[vlined,algochapter,ruled]{algorithm2e}
%\usepackage[vlined,ruled]{algorithm2e}
%\SetKwComment{Comment}{$\triangleright\ $}{}

%%% Put your definitions there:
\startlocaldefs

\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\algnewcommand\algorithmicswitch{\textbf{switch}}
\algnewcommand\algorithmiccase{\textbf{case}}
\algnewcommand\algorithmicdefault{\textbf{default}}
\algdef{SE}[SWITCH]{Switch}{EndSwitch}[1]{\algorithmicswitch\ #1\ \algorithmicdo}{\algorithmicend\ \algorithmicswitch}%
\algdef{SE}[CASE]{Case}{EndCase}[1]{\algorithmiccase\ #1}{\algorithmicend\ \algorithmiccase}%
\algdef{SE}[DEFAULT]{Default}{EndDefault}[1]{\algorithmicdefault\ #1}{\algorithmicend\ \algorithmicdefault}%
\algtext*{EndSwitch}%
\algtext*{EndCase}%
\algtext*{EndDefault}%
\renewcommand{\algorithmicensure}{\textbf{Result:}}

\newcommand*\NEPowerset{\mathcal{P}_{\geq 1}}
\newcommand*\gap{\textrm{(--)}}
\newcommand*\NonNegReals{\mathbb{R}_{\geq 0}}

\DeclarePairedDelimiter\setsize{\lvert}{\rvert}%

\endlocaldefs

	
\begin{document}
%%% Start of article front matter
\begin{frontmatter}
\begin{fmbox}
	\dochead{Methodology}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%                                          %%
	%% Enter the title of your article here     %%
	%%                                          %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\title{Efficient Implied Alignment}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%                                          %%
	%% Enter the authors here                   %%
	%%                                          %%
	%% Specify information, if available,       %%
	%% in the form:                             %%
	%%   <key>={<id1>,<id2>}                    %%
	%%   <key>=                                 %%
	%% Comment or delete the keys which are     %%
	%% not used. Repeat \author command as much %%
	%% as required.                             %%
	%%                                          %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\author[
	addressref={aff1},
	email={academia@recursion.ninja}
	]{\inits{AJW} \fnm{Alex J} \snm{Washburn}}
	\author[
	addressref={aff1},
	email={wheeler@amnh.org}   % email address
	]{\inits{WCW}\fnm{Ward C} \snm{Wheeler}}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%                                          %%
	%% Enter the authors' addresses here        %%
	%%                                          %%
	%% Repeat \address commands as much as      %%
	%% required.                                %%
	%%                                          %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\address[id=aff1]{%                           % unique id
		\orgname{Division of  Invertebrate Zoology, American Museum of Natural History}, % university, etc
		\street{200 Central Park West},                     %
		\postcode{10024-5192}                                % post or zip code
		\city{New York, NY},                              % city
		\cny{USA}                                    % country
	}

\end{fmbox}% comment this for two column layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Abstract begins here                 %%
%%                                          %%
%% Please refer to the Instructions for     %%
%% authors on http://www.biomedcentral.com  %%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}
	
	\begin{abstract}
		Given a binary tree $\mathcal{T}$ of $n$ leaves, each leaf labeled by a string of length at most $k$, and a binary string alignment function $\otimes$, an implied alignment can be generated to describe the alignment of a dynamic homology for $\mathcal{T}$.
		This is done by first decorating each node of $\mathcal{T}$ with an alignment context using $\otimes$, in a post-order traversal, then, during a subsequent pre-order traversal, inferring from those internal node decorations on which edges insertion and \texttt{LEFT}--tagged elements occurred.
		Previous descriptions of the implied alignment algorithm suggest a technique of ``back-propagation'' with time complexity $\mathcal{O}(k^2 * n^2)$.
		Here we describe an implied alignment algorithm with complexity \textsc{$\mathcal{O}(k * n^2)$}.
		For well-behaved data, such as molecular sequences, the runtime approaches the best-case complexity of \textsc{$\Omega(k * n)$}.
		This reduction in the time complexity of the procedure dramatically improves both its utility in generating multiple sequence alignments and its heuristic utility.
	\end{abstract}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%                                          %%
	%% The keywords begin here                  %%
	%%                                          %%
	%% Put each keyword in separate \kwd{}.     %%
	%%                                          %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\begin{keyword}
		\kwd{dynamic homology}
		\kwd{implied alignment}
		\kwd{multiple string alignment}
		\kwd{phylogenetics}
		\kwd{sequence alignment}
		\kwd{tree alignment}
	\end{keyword}
	
	% MSC classifications codes, if any
	%\begin{keyword}[class=AMS]
	%\kwd[Primary ]{}
	%\kwd{}
	%\kwd[; secondary ]{}
	%\end{keyword}
	
\end{abstractbox}
%
%\end{fmbox}% uncomment this for twcolumn layout

\end{frontmatter}

\newpage


\section*{Background}

Implied Alignment (IA) was proposed by \cite{Wheeler2003} as an adjunct to Direct Optimization (DO) \citep{Wheeler1996,VaronandWheeler2012} to be used in phylogenetic tree search providing both verification and more rapid heuristic analysis.
The method was originally implemented in later versions of MALIGN \citep{Wheeler1991-1998} and has been a component of POY \citep{Wheeleretal2015, POY2, POY3, POY5, Wheeleretal2006} since its inception.
A more formal description of the algorithm was presented in \cite{Wheeleretal2006} and \cite{VaronandWheeler2012}.
Although originally designed for alignment-free phylogenetic analysis (dynamic homology, \citealp{Wheeler2001}), the procedure was first used as a stand-alone multiple sequence alignment (MSA) tool by \cite{WhitingAetal2006} in their analysis of skink systematics.

Whiting et al. found that IA was superior (in terms of tree optimality score) to other MSA methods in both parsimony and likelihood analyses.
This observation has been repeated multiple times (e.g. \citealp{LindgrenandDaly2007, Lehtonen2008, WheelerandGiribet2009, FordandWheeler2015}; summarized in \citealp{Wheeler2012}).
The use of IA as an MSA algorithm as well as its use in the ``static approximation'' procedure \citep{Wheeler2003b} benefits greatly from improvements in the time complexity we present in this paper.

In a broader context, IA is a heuristic solution to the NP-hard Tree Alignment Problem (TAP) defined by \cite{sankoff1975}.
As such, any individual IA is not guaranteed to be either optimal or unique, with potentially an exponential number of equally optimal implied alignments for any given binary tree.
IA was originally described in the context of parsimony-based phylogenetic analysis and was later extended to probabilistic model-based approaches by \cite{Wheeler2006} and its implementations were described by \citep{Varonetal2010,Wheeleretal2015}.
Similar MSA approaches also based in probabilistic analysis have been described e.g by \cite{Loytynoja2005} and \cite{Paten2008}, and implemented in PRANK \citep{PRANK}.

The IA algorithm presented in this paper takes a different intellectual approach to deriving alignments than earlier versions of IA.
Previous algorithmic approaches relied on DO assigning median sequences to the graph vertices. These sequences were then consumed by IA to produce the full alignment.
Here, we describe IA through the assigning ``preliminary contexts'' to the graph vertices, and these \textit{contexts} are later consumed to produce the median sequences and the full alignment.

The algorithm uses the repeated application of a pairwise string alignment function to perform an efficient MSA for a given binary tree whose leaves are labeled by strings, i.e. the tree describes the relationship of those strings.
The more similar the initial leaf labelings the better the algorithm performs.
Thus, while this algorithm has general use for performing an MSA, it is especially well-suited for the alignment of biological sequences where the strings are highly similar and a binary tree describing the strings' relationships can be provided. 
Below, we provide an example of the IA algorithm's performance on biological data.


\section*{Definition of the heuristic function}

In order for an MSA to be inferred, there are constraints on the heuristic alignment function used to decorate the tree prior to performing the IA algorithm.
As long as these constraints are satisfied, the implementation details of the function are agnostic to the IA algorithm presented here.

Let $\Sigma$ be a finite alphabet of symbols such that $\setsize{\Sigma} \geq 3$.
Let $\gap \in \Sigma$ be a gap symbol which will have a special meaning in the context of an alignment.
Let $\NEPowerset (\operatorname{X})$ denote the powerset of $\mathrm{X}$, minus the empty set.
Let $\Sigma_{\Gamma}$ be the alphabet of the following symbols:

\begin{align*}
  \Sigma_{\Gamma} &      =      \textbf{\texttt{BOTH}}  \;\;\;\; \NEPowerset (\Sigma) \;\; \NEPowerset (\Sigma)
\\                & \;\, | \;\; \textbf{\texttt{LEFT}} \;\;\;\;  \NEPowerset (\Sigma) 
\\                & \;\, | \;\; \textbf{\texttt{RIGHT}} \;\; \quad\quad\quad\,    \;\; \NEPowerset (\Sigma)
\\                & \;\, | \;\; \textbf{\texttt{GAPPED}}
\end{align*}

That is, $\Sigma_{\Gamma}$ contains all pairs of elements of $\NEPowerset (\Sigma)$ tagged as $\textbf{\texttt{BOTH}}$, all elements of $\NEPowerset (\Sigma)$ tagged as $\textbf{\texttt{LEFT}}$, all elements of $\NEPowerset (\Sigma)$ tagged as $\textbf{\texttt{RIGHT}}$, and an additional element $\textbf{\texttt{GAPPED}}$.
This construction of $\Sigma_{\Gamma}$ extends the original alphabet $\Sigma$ to preserve alignment information in the algorithms presented below.
Note that if $\setsize{\Sigma} = x$ then $\setsize{\Sigma_{\Gamma}} = 2^{2x}$.
This follows from the fact that $|\NEPowerset (\Sigma)|$ is equal to one less than the size of the power set of $\Sigma$, due to $\NEPowerset (\Sigma)$ disallowing the empty set. 

Let $\Sigma^{*}_{\Gamma}$ be the set of all finite strings over the alphabet $\Sigma_{\Gamma}$.
Let $\otimes : \Sigma^{*}_{\Gamma} \times \Sigma^{*}_{\Gamma} \rightarrow \left(\NonNegReals,\; \Sigma^{*}_{\Gamma}\right)$ be a heuristic function that returns a nonnegative alignment cost and an alignment result in $\Sigma^{*}_{\Gamma}$.
It is required that $\otimes$ be commutative but it need not be associative.
Both of these constraints will be explored later in the ``Discussion'' section.
These constraints are necessary but not sufficient for a heuristically optimal implied alignment to be inferred on the alignment function.

It is worth noting the motivation of the constructions defined above. 
Most pairwise string alignment functions take two finite strings of symbols from the original alphabet and supply a new finite string of symbols from the original alphabet.
We can represent this class of pairwise string alignment functions by letting $\Sigma^{*}$ be the set of all finite strings over the alphabet $\NEPowerset (\Sigma)$ and letting $\odot: \Sigma^{*} \times \Sigma^{*} \rightarrow \left(\NonNegReals,\; \Sigma^{*} \right)$.
The results of $\odot$ can contain cases of ambiguity where it cannot be inferred which input elements correspond to which output elements, but the construction of $\otimes$ never produces these cases of ambiguity due to the tagging of each element.
The preservation of this relationship between input and output is required for the algorithmic improvements presented below.


\section*{Overview of the Implied Alignment Algorithm}

The IA algorithm provides an MSA for a binary tree $\mathcal{T}$ of $n$ leaves, each leaf containing a string with symbols in $\Sigma$ and length at most $k$.
To do so, we will traverse $\mathcal{T}$ twice.
First, we perform a post-order traversal---i.e. a node is visited after its children---from the leaves to the root, assigning the results of $\otimes$ as a ``preliminary context'' decoration to each node.
Second, we perform a pre-order traversal, from the root to the leaves, aligning each preliminary context with its parent to assign a ``final alignment'' decoration to each node.

The difficult part of using a binary string alignment function (like $\otimes$) to produce an MSA efficiently lies in how the output of binary operations are combined across the ``global scope'' of $\mathcal{T}$.
When performing the post-order traversal, the only information known at a given node is what is contained in its subtree.
Therefore, information for the entirety of $\mathcal{T}$ is only known at completion of the post-order traversal.
When performing the subsequent pre-order traversal, we take the ``complete'' scope available at the root node and thread the information towards the leaves.
At each post-order step we take the ``complete'' context threaded from the root and combine it with the preliminary context to assign the final alignment on that node.
Thus, we collect all requisite information for an MSA during the post-order traversal and then apply that information during the pre-order traversal to derive the MSA.

As noted above, the time complexity of the IA algorithm's pre-order traversal in previous work was $\mathcal{O}(k^2 * n^2)$.
We are able to improve this by, during the post-order pass, tagging each element of a string on a node $v_x$ with information that notates on which subtree of $v_x$ that element originated.
We can lift each symbol in $\Sigma$ into $\Sigma_{\Gamma}$ through the alignment process.
After the post-order traversal each node in $\mathcal{T}$ will have a string in $\Sigma_{\Gamma}^{*}$.
The elements of said strings are tagged with one of four options representing which child node the information of this element originated relative to that node.
Those tagged \textbf{\texttt{BOTH}} originated from both subtrees, those tagged \textbf{\texttt{LEFT}} originated from the left subtree, those tagged \textbf{\texttt{RIGHT}} originated from the right subtree, and those tagged \textbf{\texttt{GAPPED}} originated from neither subtree (elsewhere in $\mathcal{T}$).
Because elements tagged \textbf{\texttt{GAPPED}} originated from neither subtree, \textbf{\texttt{GAPPED}} elements cannot be created during the post-order, only added during the pre-order.

This tag on each element provides contextual information that allows for an efficient processing of the elements in the pre-order traversal.
During the pre-order traversal, a node's preliminary context is ``zipped'' with the parent's alignment in order to derive its final alignment.
We will show that this tagging and ``zipping'' process is a substantial improvement over previous work, reducing the time complexity from quadratic to linear in the length of the strings.
It is worth noting that this tagging can be represented as a succinct data structure per \cite{Jacobson1988}, requiring only an two additional bits per element.


\section*{An example heuristic function}

We will provide an example definition of $\otimes$ in Algorithm \ref{Alg:pairwiseAlignment} sufficient for the IA algorithm, though there are other sufficient definitions of $\otimes$.
The candidate function fitting the description of $\otimes$ we present will be defined similarly to the Needleman-Wunsch \citep{Needleman1970} algorithm for pairwise string alignment.
The algorithm is modified along same the lines that DO modified the dynamic programming technique of \cite{Sankoff2000}, with an additional step taken to produce the tagged elements in the output alignment. Algorithm \ref{Alg:pairwiseAlignment} (described below) is used to generate the results presented in the Methods section.

First, we decide deterministically which of the two input strings is assigned to the top (columns) of the alignment matrix and which string is assigned to the left side (rows).
We assign the input strings based on the data they contain.
The longer string is assigned to the columns of the alignment matrix, the shorter string to the rows.
If the strings are the same length, we take the first string under the lexical ordering of their elements and assign it to the columns and assign the second string to the rows of the alignment matrix. 
In the case that the strings are identical, the alignment is trivial.
If the first string supplied to $\otimes$ was not assigned to the rows of the matrix, then we must swap the $\textbf{\texttt{LEFT}}$ and $\textbf{\texttt{RIGHT}}$ tags of the resulting string alignment before returning the result.
This assignment ensures the commutativity of $\otimes$, which is necessary to enforce consistency of the IA algorithm.
Commutativity of $\otimes$ ensures that the IA algorithm provides the same alignment results for isomorphic tree labeling (i.e. ensures label invariance).
 
We now apply a memoized update procedure \citep{cormen2001}, a common element of dynamic programming algorithms such as the Needleman-Wunsch.
The subsequent ``traceback,'' however, is notably modified from the original Needleman-Wunsch procedure.
The upward, leftward, and diagonal directional arrows used to produce the alignment are additionally used to tag each element as $\textbf{\texttt{LEFT}}$, $\textbf{\texttt{RIGHT}}$, or $\textbf{\texttt{BOTH}}$, respectively. 
These tagged pairwise alignments will be consumed on the subsequent pre-order traversal of $\mathcal{T}$ when merging preliminary contexts.
Storing this information for each element of the pairwise alignment allows a more efficient generation of the subsequent multiple string alignment, allowing for an asymptotic improvement over the previous IA algorithm.
This additional tagging detail is the key difference between previous alignment methods and the one presented in this paper.

The example $\otimes$ presented in Algorithm \ref{Alg:pairwiseAlignment} is of $\Theta\left( k^2 \right)$ complexity in both time and space, where $k$ is the length of the longer string.
For clarity, while this example function is presented as a modification of the well understood Needleman-Wunsch algorithm (without explicit memoization), this tagging approach can be incorporated into more sophisticated pairwise string alignment algorithms.
For instance, by using the method described by \cite{Ukkonen1985100}, this algorithm's time complexity could be improved to $\mathcal{O}\left( k * s \right)$, where $s$ is the edit distance between the strings. 
Alternatively, by using the method described by \cite{Hirschberg1975}, this algorithm could be improved to use $\mathcal{O}\left( k \right)$ space.
Affine gap models \cite{Gotoh1982} can also be incorporated via the method of \cite{VaronandWheeler2012}.

The operator $\sigma \,:\, \NEPowerset(\Sigma) \times \NEPowerset(\Sigma) \rightarrow \left(\NonNegReals,\, \NEPowerset(\Sigma) \right)$ presented in throughout the pseudocode represents a metric for determining the transition cost between symbols in $\NEPowerset (\Sigma)$.
The metrics used in our data sets can be found in Table \ref{Tab:Metrics}. The metrics presented in Table \ref{Tab:Metrics} show the transition cost between elements of $\Sigma$.
However, these metrics can be expanded to define the transition cost between elements of $\NEPowerset(\Sigma)$ in the manner described by \cite{VaronandWheeler2012}.
Note that $\sigma$ can also be a more complex metric than those presented here, for instance a metric with affine or logarithmic affine gap costs, and be compatible with the IA algorithm. For usage of $\sigma$, see Algorithms \ref{Alg:pairwiseAlignment}, \ref{Alg:matrixDefinition}, and \ref{Alg:matrixTraceback}.


\section*{Description of post-order traversal}

The post-order traversal (leaves to the root) of the binary tree $\mathcal{T}$ is a straightforward procedure, see Algorithm \ref{Alg:post-order}.
We assign preliminary contexts and costs to each node, $v_x$, of $\mathcal{T}$.
These preliminary contexts will be consumed to assign a final alignment in the subsequent pre-order traversal of the tree.
The post-order traversal described here is very similar to the DO post-order traversal described by \cite{Wheeler2003}, differing only in the use of $\otimes$ which captures the preliminary context of a subtree, instead of generating a preliminary median string assignment.

First, for each leaf node, $v_x$, we set $v_x.cost$ to $0$.
Additionally, if $v_x$ is of type $\Sigma^{*}$ and not of type $\Sigma^{*}_{\Gamma}$---i.e. if it has been decorated with a finite string of symbols from the alphabet $\Sigma$, and it is not decorated with a finite string of preliminary contexts over the alphabet $\Sigma_{\Gamma}$---then we call $\Call{InitializeString}{v_x.prelimString}$ to apply the transformation $\Sigma^{*} \rightarrow \Sigma^{*}_{\Gamma}$.

On each internal node, $v_y$ with children $v_l$ and $v_r$, of $\mathcal{T}$, we call $v_l$ $\otimes$ $v_r$.
The resultant \textit{prelimString} is assigned to $v_y.prelimString$, and the sum of the $v_l.cost$, $v_r.cost$, and the alignment cost of $v_l$ $\otimes$ $v_r$ is assigned to $v_y.cost$.
By performing this operation in a post-order traversal over $\mathcal{T}$, we propagate the preliminary contexts and costs returned from the calls to $\otimes$ from the leaves to the root.

Upon completion of the post-order traversal, each internal node contains the preliminary context information and the cost for the corresponding subtree.
Consequently, when the post-order traversal is complete, the root node contains the preliminary context information of the full leaf set of strings and the alignment cost for the entire tree $\mathcal{T}$.
In the pre-order traversal, we will consume this preliminary context to perform an (efficient) alignment on the strings.

Because the post-order traversal can be performed using any valid definition of $\otimes$, the complexity of the post-order traversal is dependent on the complexity of the heuristic alignment function used.
Let the complexity of $\otimes$ be defined as $H(k)$, where $k$ is the maximum string length of the leaf labels of the tree $\mathcal{T}$.
Then post-order traversal runs in $\mathcal{O}(H(k) * n)$ time and space, where $n$ is the number of leaves in the binary tree $\mathcal{T}$.
If we were to use Ukkonen's method with the $\otimes$ described in Algorithm \ref{Alg:pairwiseAlignment}, the post-order traversal would run in $\mathcal{O}(k * s * n)$ time and space, where $s$ is the maximum edit distance between any two strings.


\section*{Description of pre-order traversal for final alignments}

The pre-order traversal (from the root to the leaves) of the binary tree $\mathcal{T}$ consumes the preliminary context decorations on each node created in the post-order traversal in order to assign final alignment decorations of $\Sigma_{\Gamma}^{*}$ to each node, see Algorithm \ref{Alg:preorder}.
First, the root node must be initialized for the pre-order traversal by assigning the final alignment as the root's preliminary context.
By initializing the root node in this manner, the root node is consistent with the treatment of any other parent node when deriving the internal node alignments in Algorithm \ref{Alg:deriveAlignment}.

For each non-root node, $v_c$, we first determine whether $v_c$ is the left or right child of its parent.
This is required because \texttt{LEFT}--tagged elements originate from alignments of the left subtree and \texttt{RIGHT}--tagged elements originate from alignments of the right subtree, and we must use this information when deriving the final alignment of $v_c$.

The final alignment of the parent of $v_c$, $v_p$, will necessarily be of greater than or equal length to $v_p$'s preliminary context, because $v_p$'s final alignment contains all the information from the contexts of $v_p$'s subtrees as well as the information from the rest of the tree, that is, the contexts of all of the subtrees of every ancestor node to $v_p$.
The preliminary context of $v_p$ is also greater than or equal to the preliminary context of $v_c$, due to the $v_p$'s context containing all information from $v_c$'s context, plus the addition of $v_c$'s sister subtree.
The resulting value assigned to $v_c$'s final alignment will have the same length as the final alignment assigned to $v_p$.
Since this invariant length is maintained from the root node to the leaf nodes' final alignment assignments, all alignments will have the same length.
This constitutes a simple inductive argument that the final alignment assignment of each node will be of equal length and constitute a genuine string alignment.

The final alignment for $v_c$ is derived by performing a ``sliding zip'' over $v_p$'s final alignment, $v_p$'s preliminary context, and $v_c$'s preliminary context.
$v_p$'s final alignment is used as the basis of the zip.
The inputs to this alignment are the preliminary contexts of $v_p$ and $v_c$ and $v_p$'s final alignment.
At each step of the ``sliding zip,'' one element of $v_p$'s final alignment will be consumed and one element of $v_c$'s final alignment will be defined.
Additionally, at each step of the zip, one of: an element from $v_p$'s preliminary context, elements from both $v_p$'s and $v_c$'s preliminary contexts, or no elements from either node's context, will be consumed.
Finally, we define an element of $v_c$'s final alignment to be either an element from $v_c$'s preliminary context or a gap.
The process is called a ``sliding zip'' because, due to the varying lengths of the three inputs, the elements of $v_p$'s and $v_c$'s preliminary contexts do not have an immediately apparent index with which they correspond to $v_p$'s final alignment, which is used as the basis of the zip.
Rather, the elements of $v_p$'s and $v_c$'s preliminary contexts ``slide'' through the zipping process, and their corresponding indices with the $v_p$'s final assignment is deduced dynamically.
The logic applied in the ``sliding zip'' is to propagate gaps from the tree above $v_c$ down to $v_c$, and when not dealing with a gap propagated from an ancestor node to $v_c$, to align the non-gap elements or introduce a new gap to be propagated.
The ``sliding zip'' process is often easier to understand by stepping through the algorithm.
An example alignment of this ``sliding zip'' process for two internal nodes is shown in Figure \ref{Fig:Examplepre-order}.

There are five cases determining the derivation of each index of $v_c$'s final alignment. The cases are presented in the pseudocode of Algorithm \ref{Alg:deriveAlignment}, in Figure \ref{Fig:Examplepre-order}, and described below.

\begin{itemize}

  \item Case 0: When the element of $v_p$'s final alignment is ``\texttt{GAPPED},'' then the next element of $v_c$'s final alignment is ``\texttt{GAPPED}.''

  \item Case 1: When the sliding zip has consumed all elements of the $v_c$'s preliminary context, then the next element of $v_c$'s final alignment is ``\texttt{GAPPED}.''
  Because we only define the next element of $v_c$'s final alignment to be either an element from $v_c$'s preliminary context or a gap, the latter is the only choice.

  \item Case 2: When the element of $v_p$'s final alignment is ``\texttt{BOTH},'' then we consume the next elements of both $v_p$'s and $v_c$'s preliminary contexts and the next element of $v_c$'s final alignment is $v_c$'s consumed preliminary context element.
  Because $v_p$'s final alignment element was marked as an alignment event, we know that $v_c$ was aligned with its sister subtree at this index, and that $v_c$'s preliminary context element is the correct element for this index of the alignment.
  
  \item Case 3: When both $v_p$'s final alignment element and $v_p$'s preliminary context element are ``\texttt{LEFT},'' and $v_c$ is the \textit{left} child of its parent, then we consume the next element of each of $v_p$'s and $v_c$'s preliminary contexts and the next element of $v_c$'s final alignment is $v_c$'s consumed preliminary context element.
  Because \texttt{LEFT}--tagged elements originate from the left subtree of a node, and $v_c$ is the left child of $v_p$, $v_c$'s preliminary context element is the correct element for this index of the alignment.
  If the same \texttt{LEFT}--tagged element was encountered but $v_c$ was the right child of $v_p$, then $v_c$'s preliminary context element would not be the correct element for this index of the alignment, because \texttt{LEFT}--tagged elements originate from the left subtree of $v_p$ and the \texttt{LEFT}--tagged element under consideration was encountered in $v_p$'s right subtree.
  In the case that a \texttt{LEFT}--tagged element is encountered in the right subtree of $v_p$, we introduce a new gap into all the alignments of the subtree at this index to account for the aligned element in the $v_c$'s sister subtree.
  This is implicitly dealt with in Case 5.
  
  \item Case 4: When both $v_p$'s final alignment element and $v_p$'s preliminary context element are ``\texttt{RIGHT},'' and $v_c$ is the \textit{right} child of $v_p$, we consume the next element of both $v_p$'s and $v_c$'s preliminary contexts and assign to the next element of $v_c$'s final alignment $v_c$'s consumed preliminary context element.
  Because \texttt{RIGHT}--tagged elements originate from the right subtree of a node, and $v_c$ is the right child of $v_p$, $v_c$'s preliminary context element is the correct element for this index of the alignment.
  If the same \texttt{RIGHT}--tagged element was encountered but $v_c$ was the left child of $v_p$, then $v_c$'s preliminary context element would not be the correct element for this index of the alignment, because \texttt{RIGHT}--tagged elements originate from the right subtree of $v_p$ and the \texttt{RIGHT}--tagged element under consideration was encountered in $v_p$'s left subtree.
  In the case that a \texttt{RIGHT}--tagged element is encountered in the left subtree of $v_p$, we introduce a new gap into all the alignments of the subtree at this index to account for the aligned element in $v_c$'s sister subtree.
  This is implicitly dealt with in Case 5.
  
  \item Case 5: When none of the conditions for Case 0, 1, 2 or 3 hold, then we consume the next element of $v_p$'s preliminary context and the next element of $v_c$'s final alignment is ``\texttt{GAPPED}.''
  This handles the cases where either the two subtrees were not aligned at the current index or a new gap needed to be introduced at the current index because a \texttt{LEFT}--tagged or \texttt{RIGHT}--tagged element was encountered in $v_p$'s right or left subtree, respectively. 

\end{itemize}

%\begin{itemize}
%
%\item If the next element of the parental final alignment is a \texttt{LEFT}--tagged element, consume the parental final alignment element and add a \texttt{LEFT}--tagged element to the current node's final alignment.
%
%\item If the next element of the parental final alignment is an insertion or alignment event and the next element of the parental preliminary context is a \texttt{LEFT}--tagged element, then we consume both elements and add a \texttt{LEFT}--tagged element to the current node's final alignment.
%
%\item If the next element of the parental final alignment is an \texttt{RIGHT}--tagged element and the next element of the parental preliminary context is an alignment event, then we consume both elements along with the next element of the current node's preliminary context and add the consumed element of the current node's preliminary context to the current node's final alignment.
%
%\item If the next element of the parental final alignment is an \texttt{RIGHT}--tagged element and the next element of the parental preliminary context is an \texttt{RIGHT}--tagged element and we are deriving the alignment of a leaf node, then we must inspect the median value of the next element of the current node's preliminary context.
%If the median value is \textit{not} the same as the inserted value from the parental preliminary context, then we consume both the parental final alignment and the parental preliminary context elements and add a \texttt{LEFT}--tagged element to the current node's final alignment. 
%If the median value is the same as the inserted value from the parental preliminary context or we are not deriving the alignment of a aligning a leaf node, then we consume all three elements along and add the consumed element of the current node's preliminary context to the current node's final alignment, converting a \texttt{LEFT}--tagged element to an \texttt{RIGHT}--tagged element if the consumed element of the current node's preliminary alignment was a \texttt{LEFT}--tagged element.
%
%\item If the next element of the parental final alignment is an alignment event and the next element of the parental preliminary context is an \texttt{RIGHT}--tagged element, then we consume both elements along with the next element of the current node's preliminary context and add the consumed element of the current node's preliminary context to the current node's final alignment.
%
%\item If the next element of the parental final alignment is an alignment event and the next element of the parental preliminary context is an alignment event, we must check if we are deriving the alignment for a leaf node.
%If we are deriving the alignment for a leaf node, then we consume both elements along with the next element of the current node's preliminary context and add the consumed element of the current node's preliminary context to the current node's final alignment.
%If we are \textit{not} deriving the alignment for a leaf node, then we consume both elements along with the next element of the current node's preliminary context and add the consumed element of the parental preliminary context to the current node's final alignment.
%
%\end{itemize}

Let $m = \frac{a}{k}$, where $k$ is the length of the longest input string, and $a$ is the length of the root node's preliminary context.
In the best case that a ``perfect alignment'' is derived, that is, that each element of all the input strings can be aligned with one of the elements of the longest input string, then $m = 1$.
In the worst case that a ``degenerate alignment'' is derived, that is, that no element of any of the input strings can be aligned with any of the elements of the longest input string, and all input strings are of equal length, then $m = n$.

The improvement of the implied alignment algorithm presented here compared to the original algorithm is that the additional stored information allows us to determine the final assignments in $\Theta(k * m * n)$ instead of $\mathcal{O}(k^2 * n^2)$ time. 
The aforementioned $n^2$ component occurred in previous implementations due to the use of a ``back-propagation'' technique, which required that, at each pre-order step, each new gap found in the alignment was retroactively applied to every alignment derived in a previous pre-order step.
Additionally, the $k^2$ component in the previous implementation was due to using a Needleman-Wunsch string alignment between the current node and its parent node at each pre-order step.
By saving the requisite information on the nodes during the post-order traversal and then consuming this information with the ``sliding-zip'' technique, we replace these computationally expensive operations with a much more efficient algorithm.

In the pre-order traversal algorithm presented above, we generate an implied alignment in $\Theta(k * m * n)$ time.
We must perform a ``sliding-zip'' operation on each node in the binary tree $\mathcal{T}$, hence the factor of $n$.
The ``sliding zip'' accounts for the $k * m$ factor.

The best case time complexity occurs when the length of the derived alignment is the length of the longest input string, an alignment with the minimal number of elements.
In this case, $m = 1$ and the ``sliding zip'' performed on each node performs work equal to the length of the longest input string $k$.
Hence, the best case time complexity of the implied alignment algorithm is $\Omega(n*k)$ when the input strings are highly correlated and $m = 1$.

The worst case time complexity occurs when the length of the derived alignment is equal to the sum of the lengths of the input strings, an alignment with the maximum number of elements.
In the worst case, $m \gg 1$, and the ``sliding zip'' performed on each node performs work equal to the length of the longest input string, $k$, multiplied by the number of input strings, $n$.
Hence, the worst case time complexity of the implied alignment algorithm is $\mathcal{O}(k * n^2)$ when the input strings are independent of each other.


\section*{Methods}

An example Haskell implementation of the implied alignment algorithm described above, the data sets used to generate the results, along with a script to replicate the results discussed below can all be found here: 

\centerline{\url{https://github.com/recursion-ninja/efficient-implied-alignment}}

To explore the performance of the pre-order traversal, we ran the implied alignment algorithm described in this paper on fungal and metazoan biological data sets described by \cite{GiribetandWheeler1999} and \cite{GiribetandWheeler2001} respectively.
Both full data sets consisted of a preselected tree and predetermined string alignment (i.e. including gaps).
The full leaf set of the tree was repeatedly halved to produce a data set of doubling leaf set sizes.
The string alignment was repeatedly truncated, dropping the beginning and end of the alignment, taking the central slice of the current length from each string, and then removing all the gaps from the alignment slice.
The pruned trees and truncated strings were used as progressively doubling inputs, to measure runtime scaling in terms of both leaf set size and string length.
Both biological data sets used the discrete metric $\sigma_0$ and the alphabet $\Sigma = \{\mathrm{A, C, G, T,}$ --$\,\}$.

Additionally, a pathological data set was constructed to illustrate the best and worst case performance of the implied alignment algorithm.
The pathological data set consisted of balanced binary trees which repeatedly doubled in size.
The smallest tree is a quartet tree, with the strings consisting of a single symbol from the alphabet $\Sigma = \{\mathrm{A, C, G, T}\}$ repeated $k$ number of times.
The lengths of the strings on each leaf were repeatedly doubled in size to scale the string length.
The size of the tree was scaled by taking $2^{\frac{n}{4}}$ quartet trees and combining them together into a larger balannced binary tree of $n$ leaves.

The time complexity scaling of this pathological data set was examined under two different metrics $\sigma_1$ and $\sigma_2$.
The former metric preferentially selects substitution events over insertion or \texttt{LEFT}--tagged elements, thus producing the ``perfect alignment.''
Conversely, the latter selects for insertion or deletion over substitution, thus producing the ``degenerate alignment.''


After running the algorithm on each data set, we constructed an Ordinary Least Square (OLS) model with the running time in milliseconds as a function of dimensions $k$ and $n$.
We took the binary logarithm, $\log_{2}$, of both input dimensions as well as the output.
From there, we calculated the coefficients of each input in this equation: $\log_2(runtime) = \beta_0 + \beta_1 \log_2(n) + \beta_2 \log_2(k) + \epsilon$, where $\epsilon$ represents the estimation error.
Note that, because the logarithm of the inputs was taken, we would expect $\beta_1$ to be close to $1$ for linear performance with respect to that input variable and close to $2$ for quadratic performance.
See Table \ref{Tab:Regression}.


\section*{Results}

The pathological data sets shows the stark difference between the best case $\Omega(n * k)$ and worst case $\mathcal{O}(k * n^2)$ performances.
The OLS model empirically supports the theoretical best and worst cases demonstrated by the two runs on the pathological data set as shown in Figures \ref{Fig:Best} and \ref{Fig:Worst}.

The OLS model also anecdotally supports the supposition that time complexity scales well for the biological data sets.
The fungal and metazoan sequence data sets demonstrate a near-linear time complexities with respect to the number of input strings and linear complexity with respect to string length.
The fungal data sets lend support to the argument that some of ``real world'' use cases can perform close to the theoretical best case complexity (see Figures \ref{Fig:Fungi} and \ref{Fig:Metazoa}).


%\begin{table}
%\scriptsize
%\begin{minipage}{0.5\textwidth}
%\centering
%\caption{Fungi pre-order results}
%\begin{tabular}{l|l|l}%
%    \bfseries $n$ & \bfseries $k$ & \bfseries Runtime (ms)% specify table head
%    \csvreader[head to column names]{fungi-11.pre-order.csv}{}% use head of csv as column names
%    {\\\hline\csvcoliii&\csvcoliv&\csvcolv}% specify your coloumns here
%\end{tabular}
%\end{minipage}
%\hfill
%\begin{minipage}{0.5\textwidth}
%\centering
%\caption{Metazoa pre-order results}
%\begin{tabular}{l|l|l}%
%    \bfseries $n$ & \bfseries $k$ & \bfseries Runtime (ms)% specify table head
%    \csvreader[head to column names]{metazoa-11.pre-order.csv}{}% use head of csv as column names
%    {\\\hline\csvcoliii&\csvcoliv&\csvcolv}% specify your coloumns here
%\end{tabular}
%\end{minipage}
%\end{table}

%\begin{table}
%\scriptsize
%\begin{minipage}{0.5\textwidth}
%\centering
%\caption{Best case pre-order results}
%\begin{tabular}{l|l|l}%
%    \bfseries $n$ & \bfseries $k$ & \bfseries Runtime (ms)% specify table head
%    \csvreader[head to column names]{pathological-12.pre-order.csv}{}% use head of csv as column names
%    {\\\hline\csvcoliii&\csvcoliv&\csvcolv}% specify your coloumns here
%\end{tabular}
%\end{minipage}
%\hfill
%\begin{minipage}{0.5\textwidth}
%\centering
%\caption{Worst case pre-order results}
%\begin{tabular}{l|l|l}%
%    \bfseries $n$ & \bfseries $k$ & \bfseries Runtime (ms)% specify table head
%    \csvreader[head to column names]{pathological-31.pre-order.csv}{}% use head of csv as column names
%    {\\\hline\csvcoliii&\csvcoliv&\csvcolv}% specify your coloumns here
%\end{tabular}
%\end{minipage}
%\end{table}

\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}


\section*{Conclusions}

The IA algorithm can be improved to run with $\mathcal{O}(k * n^2)$ and best case $\Omega(k * n)$ complexity of time and space. The more similar the input strings are, the closer the performance will be to the best case. When the algorithm is applied to ``real world'' biological sequences, the performance tends strongly towards the best case. The improved algorithm presented in this paper offers immediate and significant gains to applications related to the TAP and MSA.


\section*{Discussion}

The algorithm originally described by \cite{Wheeler2003} was given the name implied alignment to differentiate it from other methods (e.g. sum-of-pairs alignment) unconnected to the vertex string assignments ``implied'' by the binary tree on a given leaf-set. 
However, it is worth articulating exactly how the alignment we derive is \emph{implied} by the tree.
In short, it is the requirement of commutativity and the lack of associativity.

For the purposes of this analysis we will ignore the cost returned from the $\otimes$ and consider only the resulting alignment context.
Therefore let $\oplus : \Sigma^{*}_{\Gamma} \times \Sigma^{*}_{\Gamma} \rightarrow \Sigma^{*}_{\Gamma}$ be defined as $\otimes$, but ignoring the alignment cost of the result. If we are given a rooted binary tree $\mathcal{T} = ((A,B),(C,D))$ with leaves $A, B, C, D \in \Sigma^{*}$ then the ancestral state of the root node defined by the heuristic function $\oplus$ would be $((A \oplus B) \oplus (C \oplus D))$. 
In fact, the ancestral state of any internal node defined by $\oplus$ can be calculated by applying $\oplus$ recursively to the subtree of the internal node.
The binary structure of the tree directly implies the precedence of each application of $\oplus$ in the final result.
Since $\oplus$ need not be associative, the tree $((A,(B,C)),D)$ evaluated as $((A \oplus (B \oplus C)) \oplus D)$, is likely to yield different results.
However, since $\oplus$ is commutative, transposing any child nodes between the left and right positions of their parent will result in a tree that yields the same internal values. 
For example consider a transposed tree $\mathcal{T'}$:

\begin{align*}
  eval(\mathcal{T'}) &= eval((D,C),(B,A))
\\  &= ((D \oplus C) \oplus (B \oplus A))
\\  &= ((C \oplus D) \oplus (B \oplus A))
\\  &= ((C \oplus D) \oplus (A \oplus B))
\\  &= ((A \oplus B) \oplus (C \oplus D))
\\  &= eval((A,B),(C,D))
\\  &= eval(\mathcal{T})
\end{align*}

This commutative property and lack of an associative property is why the alignment is implied by the tree on the leaf-set under $\oplus$ and not the unique alignment on all trees for the leaf-set under $\oplus$.
Clearly, a $\oplus$ that is both commutative \emph{and} associative using the algorithm described in this paper would yield the same alignment on all trees for a given leaf-set.

%String alignment is of paramount importance in phylogenetic search, though \textit{when} the string alignment occurs can be a matter of contention.
%Traditionally all input sequence strings are aligned first, and that alignment is held constant as a phylogenetic tree is searched for on this alignment.
%An alternate method based on dynamic homology has been described by \citep{Wheeler1996}, in which strings are not pre-aligned, but rather are compared vertex by vertex on the current tree under consideration during a phylogenetic search.
%Although implied alignments are not required to identify phylogenetic trees that are heuristically ``best,'' they can be used in enhanced tree search heuristics during a dynamic homology search (as in POY \citealp{POY5, Wheeleretal2015}) or as the basis of a search itself, as with more traditional approaches.
%
%How a string alignment is used in a phylogenetic search leads to an important distinction in interpreting the meaning of the resulting string alignment.
%When performing the traditional methodology of aligning strings then searching on the static alignment, the resulting topology of the phylogenetic search is implied by the initial alignment.
%When using implied alignment to search on a dynamic homology, the alignments depend on the topology.

%See figure from this other paper Ward will note, as a good example of how Implied Alignment differs from a traditional multiple sequence alignment algorithm.


%\section*{Future work}

%If a heuristic function $\otimes$ that was both commutative and associative and with sufficiently low error could be defined, then the iterative approximation \citep{Wheeler2003a} could be dramatically improved upon. 
%Even an incredibly expensive, poly-time $\otimes$ definition would have its cost amortized over the tree-space search because the heuristic would only need to be applied once to a dynamic homology and then an implied alignment generated, which would be the same for all trees in the tree space. 
%The iterative approximation would cease needing to be iterative and rather be an \emph{invariant} approximation that could be computed once and referenced many times.
%The proposed invariant approximation would have heuristic utility as a basis for  comparing on which trees in the tree space a more computationally expensive implied alignment should be performed.


%\section{Appendix}
%
%\begin{table}
%	\scriptsize
%	\caption{Fungi pre-order results (milliseconds)}
%	\begin{tabular}{R{0.6cm}|R{1cm}R{1cm}R{1cm}R{1cm}R{1cm}R{1.1cm}}%
%		\bfseries $n\backslash k$ &   364 &    728 &   1456 &   2913 &    5826 &   11652 \\\hline     
%		25  &   730 &   1399 &   2072 &   2664 &    5691 &    7683 \\
%		50  &  1385 &   2214 &   3497 &   5696 &   23837 &   16300 \\
%		100  &  4133 &   5301 &   9498 &  14467 &   24514 &   34216 \\
%		200  &  7238 &  11577 &  18303 &  23809 &   57262 &   78167 \\
%		400  & 12092 &  24190 &  35627 &  66681 &  140659 &  214599 \\
%		800  & 36959 &  52147 & 101514 & 217350 &  333379 &  709528 \\
%		1553  & 86631 & 137355 & 310405 & 487501 & 1462000 & 1743000
%		\label{Tab:Fungi}
%	\end{tabular}
%	
%	%\end{table}
%	%\begin{table}
%	\scriptsize
%	\caption{Metazoa pre-order results (milliseconds)}
%	\begin{tabular}{R{0.6cm}|R{1cm}R{1cm}R{1cm}R{1cm}R{1cm}R{1.1cm}}%
%		\bfseries $n\backslash k$ &   1134 &   2269 &   4539 &    9079 &   18158 &   36317 \\\hline     
%		25  &    420 &    612 &   1068 &    3401 &    5902 &    9787 \\
%		50  &    979 &   1543 &   3074 &    8333 &   15272 &   27715 \\
%		100  &   1824 &   3668 &   7526 &   17478 &   33379 &   64810 \\
%		200  &   3874 &  12721 &  29071 &   56608 &   97899 &  187654 \\
%		400  &  14621 &  29731 &  63591 &  162526 &  347680 &  501298 \\
%		800  &  49024 &  88415 & 225158 &  503264 &  787639 & 1670000 \\
%		1766  & 194138 & 365731 & 726205 & 1533000 & 2889000 & 6925000
%		\label{Tab:Metazoa}
%	\end{tabular}
%	%\end{table}
%	%\begin{table}
%	\scriptsize
%	\caption{Best case pre-order results (milliseconds)}
%	\begin{tabular}{R{0.6cm}|R{1cm}R{1cm}R{1cm}R{1cm}R{1cm}}%
%		\bfseries $n\backslash k$ &     25 &     50 &    100 &     200 &     400 \\\hline     
%		4  &     21 &    118 &     46 &     118 &     270 \\
%		8  &     50 &     49 &    103 &     207 &     382 \\
%		16  &     54 &    130 &    203 &     400 &     887 \\
%		32  &    138 &    233 &    446 &     925 &    1786 \\
%		64  &    249 &    466 &    881 &    1769 &    3584
%		\label{Tab:Best}
%	\end{tabular}
%	%\end{table}
%	%\begin{table}
%	\scriptsize
%	\caption{Worst case pre-order results (milliseconds)}
%	\begin{tabular}{R{0.6cm}|R{1cm}R{1cm}R{1cm}R{1cm}R{1cm}}%
%		\bfseries $n\backslash k$ &     25 &     50 &    100 &     200 &     400 \\\hline     
%		4  &    127 &     60 &    165 &     244 &     604 \\
%		8  &    336 &    240 &   1223 &    1002 &    2167 \\
%		16  &    994 &    997 &   1937 &    3896 &    8369 \\
%		32  &   1726 &   3938 &   7070 &   15197 &   33148 \\
%		64  &   6743 &  13620 &  28416 &   63780 &  145712
%		\label{Tab:Worst}
%	\end{tabular}
%\end{table}


\section*{Declarations}

  \subsection*{Ethics approval and consent to participate}

    Not Applicable
    
  \subsection*{Consent for publication}

    Not applicable   

  \subsection*{Competing interests}

    The authors declare that they have no competing interests

  \subsection*{Availability of data and materials}
  
    The datasets generated and analysed in the study are available in the GitHub.com repository,
    
    \centerline{\url{https://github.com/recursion-ninja/efficient-implied-alignment}}

  \subsection*{Authors' contributions}
  
    AW developed the algorithmic improvements, implemented the prototype program, quantified the relationship between similar inputs and improved performance.
    WW provided empirical data sets, developed and implemented pruning methodologies for the scaling of the data sets, and performed literature review.
    Both authors read and approved the final manuscript.
    
  \subsection*{Funding}

    This work was supported by DARPA SIMPLEX (``Integrating Linguistic, Ethnographic, and Genetic Information of Human Populations: Databases and Tools,'' DARPA-BAA-14-59 SIMPLEX TA-2, 2015-2018) and Robert J. Kleberg Jr. and Helen C. Kleberg foundation grant ``Mechanistic Analyses of Pancreatic Cancer Evolution''.

  \subsection*{Acknowledgements}
	
    We would like to thank Eric Ford, Callan McGill, Katherine St. John, and Erilia Wu for insightful discussions. We would also like to thank the three reviewers for improvements to the manuscript.

\begin{backmatter}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%                  The Bibliography                       %%
	%%                                                         %%
	%%  Bmc_mathpys.bst  will be used to                       %%
	%%  create a .BBL file for submission.                     %%
	%%  After submission of the .TEX file,                     %%
	%%  you will be prompted to submit your .BBL file.         %%
	%%                                                         %%
	%%                                                         %%
	%%  Note that the displayed Bibliography will not          %%
	%%  necessarily be rendered by Latex exactly as specified  %%
	%%  in the online Instructions for Authors.                %%
	%%                                                         %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	% if your bibliography is in bibtex format, use those commands:
	\bibliographystyle{bmc-mathphys} % Style BST file (bmc-mathphys, vancouver, spbasic).
	\bibliography{big-refs-3}
	% for author-year bibliography (bmc-mathphys or spbasic)
	% a) write to bib file (bmc-mathphys only)
	% @settings{label, options="nameyear"}
	% b) uncomment next line
	%\nocite{label}
	
	% or include bibliography directly:
	% \begin{thebibliography}
	% \bibitem{b1}
	% \end{thebibliography}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%                               %%
	%% Figures                       %%
	%%                               %%
	%% NB: this is for captions and  %%
	%% Titles. All graphics must be  %%
	%% submitted separately and NOT  %%
	%% included in the Tex document  %%
	%%                               %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	%%
	%% Do not use \listoffigures as most will included as separate files
	
    \newpage
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%                               %%
    %% Tables                        %%
    %%                               %%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
    %% Use of \listoftables is discouraged.
    %%
    \section*{Tables}
    
    \begin{table}[!hbt]
    	\caption{Metric costs of $\sigma_0$, $\sigma_1$, and $\sigma_2$}
    	\label{Tab:Metrics}
    	\begin{minipage}{0.3\textwidth}
    		\centering
    		\begin{tabular}{c|ccccc}
    			$\sigma_0$ & A & C & G & T & -- \\ \hline
    			A  & 0 & 1 & 1 & 1 & 1  \\
    			C  & 1 & 0 & 1 & 1 & 1  \\
    			G  & 1 & 1 & 0 & 1 & 1  \\
    			T  & 1 & 1 & 1 & 0 & 1  \\
    			-- & 1 & 1 & 1 & 1 & 0
    		\end{tabular}
    		%	\caption{Discrete}
    	\end{minipage}
    	\hfill
    	\begin{minipage}{0.33\textwidth}
    		\centering
    		\begin{tabular}{c|ccccc}
    			$\sigma_1$ & A & C & G & T & -- \\ \hline
    			A  & 0 & 1 & 1 & 1 & 2  \\
    			C  & 1 & 0 & 1 & 1 & 2  \\
    			G  & 1 & 1 & 0 & 1 & 2  \\
    			T  & 1 & 1 & 1 & 0 & 2  \\
    			-- & 2 & 2 & 2 & 2 & 0
    		\end{tabular}
    		%\caption{Prefer Align}
    	\end{minipage}
    	\hfill
    	\begin{minipage}{0.3\textwidth}
    		\centering
    		%\begin{table}[!hbt]
    		%\begin{center}
    		\begin{tabular}{c|ccccc}
    			$\sigma_2$ & A & C & G & T & -- \\ \hline
    			A  & 0 & 3 & 3 & 3 & 1  \\
    			C  & 3 & 0 & 3 & 3 & 1  \\
    			G  & 3 & 3 & 0 & 3 & 1  \\
    			T  & 3 & 3 & 3 & 0 & 1  \\
    			-- & 1 & 1 & 1 & 1 & 0
    		\end{tabular}
    		%\caption{Prefer InDel}
    	\end{minipage}\par
        \vspace{0.75em}\raggedright
        Expansion of the metrics presented in Table \ref{Tab:Metrics} is described by \cite{VaronandWheeler2012}.
    \end{table}
    
	\begin{table}[!htbp] \centering 
		\caption{Regression coefficients of leaf-set size and string length on runtime} 
		\label{Tab:Regression} 

	\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
			\\[-1.8ex]\hline 
			\hline \\[-1.8ex] 
			& \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
			\cline{2-5} 
			\\[-1.8ex] & \multicolumn{4}{c}{$\log_2$ (Runtime)} \\ 
			& \multicolumn{1}{c}{Best} & \multicolumn{1}{c}{Worst} & \multicolumn{1}{c}{Fungi} & \multicolumn{1}{c}{Metazoa} \\ 
			\\[-1.8ex] & \multicolumn{1}{c}{(1)} & \multicolumn{1}{c}{(2)} & \multicolumn{1}{c}{(3)} & \multicolumn{1}{c}{(4)}\\ 
			\hline \\[-1.8ex] 
			$\log_2$ (String count $\;n$) & 1.041 & 1.948 & 1.196 & 1.524 \\ 
			& & & & \\ 
			$\log_2$ (String length $k$) & 0.901 & 1.013 & 0.796 & 1.040 \\ 
			& & & & \\ 
			\hline \\[-1.8ex] 
			Observations & \multicolumn{1}{c}{25} & \multicolumn{1}{c}{25} & \multicolumn{1}{c}{42} & \multicolumn{1}{c}{42} \\ 
			Adjusted R$^{2}$ & \multicolumn{1}{c}{0.972} & \multicolumn{1}{c}{0.996} & \multicolumn{1}{c}{0.984} & \multicolumn{1}{c}{0.995} \\ 
			\hline 
			\hline \\[-1.8ex] 
		\end{tabular} 
	\end{table}

	\section*{Figures}
	
	\begin{figure}[h!]
		\caption{\csentence{Example pre-order alignment for a parent node and it's left child.}
			\newline Demonstrates how each of the different cases of the pre-order algorithm are applied in the ``sliding-zip'' procedure. See Algorithm \ref{Alg:deriveAlignment}.
			\newline
			The cell with the ``\textbf{\texttt{?}}'' is the cell who's value is being computed.
			\newline
			Cells with ``\textbf{\texttt{B}}'' are elements tagged as \textbf{\texttt{BOTH}}.
			\newline 
			Cells with ``\textbf{\texttt{L}}'' are elements tagged as \textbf{\texttt{LEFT}}.
			\newline 
			Cells with ``\textbf{\texttt{R}}'' are elements tagged as \textbf{\texttt{RIGHT}}.
			\newline
			Cells with ``\textbf{\texttt{G}}'' are elements tagged as \textbf{\texttt{GAPPED}}.
			\newline
			Cells with ``\textbf{\texttt{X}}'' represent that the end of the sequence has been reached.
		}
		\label{Fig:Examplepre-order}
		\vspace{2ex}
		\includegraphics{example-preorder.eps}
	\end{figure}
	
%	\begin{figure}[h!]
%			\caption{Fungi pre-order runtime}
%			\label{Fig:Fungi}
%			\includegraphics[width=1.05\textwidth]{fungi-preorder.png}
%	\end{figure}
%
%    \begin{figure}[h!]
%			\caption{Metazoa pre-order runtime}
%			\label{Fig:Metazoa}
%			\includegraphics[width=1.05\textwidth]{metazoa-preorder.png}
%	\end{figure}
%	
%    \begin{figure}[h!]
%			\caption{Best case pre-order runtime}
%			\label{Fig:Best}
%			\includegraphics[width=1.05\textwidth]{pathological-12-preorder.png}
%	\end{figure}
%
%    \begin{figure}[h!]
%			\caption{Worst case pre-order runtime}
%			\label{Fig:Worst}
%			\includegraphics[width=1.05\textwidth]{pathological-31-preorder.png}
%	\end{figure}
	
	\begin{figure}[h]
		\centering
		\begin{minipage}{0.48\textwidth}
			\centering
			\caption{Best case pre-order runtime}
			\label{Fig:Best}
			\includegraphics[width=1.1\textwidth]{pathological-12-preorder.png}
		\end{minipage}
		\hfill
		\begin{minipage}{0.48\textwidth}
			\centering
			\caption{Worst case pre-order runtime}
			\label{Fig:Worst}
			\includegraphics[width=1.1\textwidth]{pathological-31-preorder.png}
		\end{minipage}

		\begin{minipage}{0.48\textwidth}
			
			\centering
			\caption{Fungi pre-order runtime}
			\label{Fig:Fungi}
			\includegraphics[width=1.1\textwidth]{fungi-preorder.png}
		\end{minipage}
		\hfill
		\begin{minipage}{0.48\textwidth}
			\centering
			\caption{Metazoa pre-order runtime}
			\label{Fig:Metazoa}
			\includegraphics[width=1.1\textwidth]{metazoa-preorder.png}
		\end{minipage}
		
	\end{figure}

	
	\section*{Algorithms}	
	
	\begin{algorithm}
		\caption{Example $\otimes$ definition}
		\label{Alg:pairwiseAlignment}
		\begin{algorithmic}[1]
			\Require{$lhs, \; rhs \in \Sigma_{\Gamma}^{*}$ }
			\Ensure{($\NonNegReals$, $\Sigma_{\Gamma}^{*}$)}
			\Function{$\otimes$}{$\textit{lhs}, \textit{rhs}$}
			\newline
			\Comment{Conditionals here are required to ensure commutativity of $\otimes$.}
			\If    {$\textit{lhs} = \textit{rhs}$}
			\State \Return $\textit{lhs}$
			\ElsIf {$\textit{lhs} < \textit{rhs}$}
			\State \Return $\Call{matrixTraceback}{\sigma, \textit{lhs}, \textit{rhs}}$ \Comment{See Algorithm \ref{Alg:matrixTraceback}}
			\Else
			\State $\textit{result} \gets \Call{matrixTraceback}{\sigma, \textit{rhs}, \textit{lhs}}$ \Comment{See Algorithm \ref{Alg:matrixTraceback}}
			\State \Return $\Call{swapContexts}{\textit{result}}$ \Comment{See Algorithm \ref{Alg:swapContext}}
			\EndIf
			\EndFunction
			
		\end{algorithmic}
	\end{algorithm}
	
	\begin{algorithm}
		\caption{Swap the $\textbf{\texttt{LEFT}}$ and $\textbf{\texttt{RIGHT}}$ contexts of $s \in \Sigma_{\Gamma}^{*}$}
		\label{Alg:swapContext}
		\begin{algorithmic}[1]
			\Require{$inputString \in \Sigma_{\Gamma}^{*}$ }
			\Ensure{$\Sigma_{\Gamma}^{*}$ }
			\Function{swapContexts}{$\textit{inputString}$}
			\State $i \gets \vert\textit{inputString} \;\vert - 1$
			\While{$i \geq 0$}
			\Switch{$\textit{inputString}_i$}
			\Case{$\textbf{\texttt{LEFT}}  \;\;\;\; v$}
			\State $\textit{inputString}_i \gets \textbf{\texttt{RIGHT}} \;\; v$
			\EndCase
			\Case{$\textbf{\texttt{RIGHT}} \;\; v$}
			\State $\textit{inputString}_i \gets \textbf{\texttt{LEFT}} \;\;\;\;  v$
			\EndCase
			\EndSwitch
			\State $i \gets i - 1$
			\EndWhile
			\Return $\textit{inputString}$
			\EndFunction
		\end{algorithmic}
    \end{algorithm}

	\begin{algorithm}
		\caption{Generate the alignment matrix of $\otimes$, presented without explicit memoization}
		\label{Alg:matrixDefinition}
		\begin{algorithmic}[1]
			\Require{$lesser, \; longer \in \Sigma_{\Gamma}^{*}$}
			\Require{$\sigma \,:\, \NEPowerset(\Sigma) \times \NEPowerset(\Sigma) \rightarrow \left(\NonNegReals,\, \NEPowerset(\Sigma) \right)$}
			\Require{$i \, \in \left[-1, \; \vert \textit{lesser} \,\;\vert - 1 \right] \subset \mathbb{Z}$}
			\Require{$j    \in \left[-1, \; \vert \textit{longer} \,  \vert - 1 \right] \subset \mathbb{Z}$}
			\Ensure{($\NonNegReals$, $\mathbf{Dir}$, $\Sigma_{\Gamma}$)}\\
			 A suitable memoization strategy should be applied to avoid repeated work of shared sub problems
			\Function{matrixDefinition}{$\textit{lesser}, \textit{longer}, \sigma, i, j$}
			\If   {$i < 0 \lor j < 0$} \Comment{Outside of matrix is infinite cost}
			\State \Return $\left( \infty, \nwarrow, \textbf{--} \right)$
			\ElsIf{$i = 0 \land j = 0$} \Comment{Handle the origin}
			\State \Return $\left( 0, \nwarrow, \textbf{--} \right)$
			\ElsIf{$j \neq 0 \land \textit{longer}_{j-1}      = \textbf{--}$} \Comment{Preserve input gap}
			\State $\left(\textit{leftCost}, \_, \_ \right) \gets \Call{matrixDefinition}{\textit{lesser}, \textit{longer}, \sigma, i, j - 1}$
			\State \Return $\left( \textit{leftCost}, \leftarrow, \textbf{--} \right)$
			\ElsIf{$i \neq 0 \land \textit{lesser}_{i-1} \,\; = \textbf{--}$} \Comment{Preserve input gap}
			\State $\left(\textit{aboveCost}, \_, \_ \right) \gets \Call{matrixDefinition}{\textit{lesser}, \textit{longer}, \sigma, i, j - 1}$
			\State \Return $\left( \textit{aboveCost}, \uparrow, \textbf{--} \right)$
			\Else \Comment{General recursive case}
			\State $x \gets \sigma \left(           \textit{longer}_{j-1}, \textit{lesser}_{i-1} \right)$
			\State $y \gets \sigma \left(           \textit{longer}_{j-1}, \quad\quad\quad \textbf{--} \right)$
			\State $z \gets \sigma \left( \quad\quad\quad\;\, \textbf{--}, \textit{lesser}_{i-1} \right)$
			\State $\left( minCost, minDir, minElem \right) \gets \Call{getMinimal}{x, y, z}$ \Comment{See Algorithm \ref{Alg:getMinimal}}
			\If {$\left( \textit{minDir}, \textit{minElem} \right) = \left( \nwarrow, \textbf{--} \right)$} \Comment{Aligned gap is insertion}
			\State \Return $\left( minCost, \leftarrow, \textbf{--} \right)$
			\Else
			\State \Return $\left( minCost, minDir, minElem \right)$
			\EndIf
			\EndIf
			\EndFunction
		\end{algorithmic}
	\end{algorithm}
	
	\begin{algorithm}
		\caption{Consume the alignment matrix of $\otimes$, returning the alignment and cost}
		\label{Alg:matrixTraceback}
		\begin{algorithmic}[1]
			\Require{$lesser,\; longer \in \Sigma_{\Gamma}^{*}$}
			\Require{$\sigma \,:\, \NEPowerset(\Sigma) \times \NEPowerset(\Sigma) \rightarrow \left(\NonNegReals,\, \NEPowerset(\Sigma) \right)$}
			\Ensure{($\NonNegReals$, $\Sigma_{\Gamma}^{*}$)}\\
			A suitable memoization strategy should be applied to avoid repeated work of shared sub problems
			\Function{matrixTraceback}{$\textit{lesser}, \textit{longer}$}
			\State $\left( i, j \right) \gets \left( \vert \textit{lesser} \;\vert - 1,\; \vert \textit{longer} \;\vert - 1 \right)$
			\State $\left( alignedCost, \_, \_ \right) \gets \Call{matrixDefinition}{\textit{lesser}, \textit{longer}, \sigma, i, j}$ \Comment{See Algorithm \ref{Alg:matrixDefinition}}
			\State $alignedStr \gets \left[ \,\right]$
			\While{$\left( i, j \right) > \left( 0, 0 \right)$}
			\State $\left( \_, dirArrow, elem \right) \gets \Call{matrixDefinition}{\textit{lesser}, \textit{longer}, \sigma, i, j}$ \Comment{See Algorithm \ref{Alg:matrixDefinition}}
			\Switch{$\textit{dirArrow}$}
			\Case{$\leftarrow$}
			\State $\left( i, j\right) \gets \left( i \quad\;\;, j - 1       \right)$
			\State $nextElement \gets \textbf{DELETE} \;\;   elem \;\; \textit{longer}_{j-1}$
			\EndCase
			\Case{$\uparrow$}
			\State $\left( i, j\right) \gets \left( i - 1      , j \quad\;\; \right)$
			\State $nextElement \gets \textbf{INSERT} \;\;\; elem \hspace{1.9cm} \textit{lesser}_{i-1}$
			\EndCase
			\Case{$\nwarrow$}
			\State $\left( i, j\right) \gets \left( i - 1      , j - 1       \right)$
			\State $nextElement \gets \textbf{ALIGN} \quad\; elem \;\; \textit{longer}_{j-1} \;\; \textit{lesser}_{i-1}$
			\EndCase
			\EndSwitch
			\State $alignedStr \gets nextElement + alignedStr$
			\EndWhile
			\State \Return $\left(alignedCost, alignedStr \right)$
			\EndFunction
		\end{algorithmic}
	\end{algorithm}
	
	\begin{algorithm}
		\caption{Get the minimal directional matrix context from the $3$ inputs}
		\label{Alg:getMinimal}
		\begin{algorithmic}[1]
			\Require{$\left(\;\;diagCost,\;\,diagElem \right) \in \left(\NonNegReals, \Sigma_{\Gamma} \right)$}
			\Require{$\left(\, rightCost,   rightElem \right) \in \left(\NonNegReals, \Sigma_{\Gamma} \right)$}
			\Require{$\left(    downCost,    downElem \right) \in \left(\NonNegReals, \Sigma_{\Gamma} \right)$}
			\Require{Total ordering over directional arrows defined as: $\nwarrow \; < \; \leftarrow \; < \; \uparrow$}
			\Ensure{$\left(\NonNegReals, \Sigma_{\Gamma} \right)$}
			\Function{getMinimal}{$ \; \left(\;\;diagCost,\;\,diagElem \right)$\newline\hspace*{3.40cm}
				$,\, \left(\, rightCost,   rightElem \right)$\newline\hspace*{3.40cm}
				$,\, \left(    downCost,    downElem \right)$\newline\hspace*{3.55cm}}
	        \If   {$\textit{diagCost}  \leq \textit{rightCost}$}
              \If {$\textit{diagCost}  \leq \textit{downCost}$}
			    \State \Return $\left(\;\;diagCost, \;\,diagElem   \quad\;\;\;\;\,\,\,   ,\;      \nwarrow    \right)$
			  \Else
			    \State \Return $\left(    downCost,     downElem \,\bigcup\, \gap, \;   \uparrow \; \right)$
			  \EndIf
			\Else
			  \If {$\textit{rightCost} \leq \textit{downCost}$}
                \State \Return $\left(\, rightCost,    rightElem \,\bigcup\, \gap, \, \leftarrow    \right)$
              \Else
                \State \Return $\left(    downCost,     downElem \,\bigcup\, \gap, \;   \uparrow \; \right)$
              \EndIf
            \EndIf
			\EndFunction
		\end{algorithmic}
	\end{algorithm}
	
	\begin{algorithm}
		\caption{Post-order Traversal}
		\label{Alg:post-order}
		\begin{algorithmic}[1]
					
			\Require{A binary tree decorated with leaf labels $inputString \in \Sigma_{\Gamma}^{*}$ }
			\Ensure{A binary tree decorated with internal labels $prelimString \in \Sigma_{\Gamma}^{*}$ }
			%    \Procedure{post-order(n)}{}
			\Function{post-order}{$\textit{node}$}
			\If   {isLeaf ( $\textit{node}$ )}
			\State $\textit{node.cost} \gets 0$
			\State $\textit{node.prelimString} \gets \Call{initializeString}{node.prelimString}$
			%        \If {isOfType ($\Sigma^{*}$, $\textit{node.prelimString}$ )}
			%          \State $\textit{node.prelimString} \gets \Call{initializeString}{node.prelimString}$
			%sy        \EndIf
			\Else
			\State $\textit{lhs}  \gets \Call{post-order}{\textit{node.children.first}}$
			\State $\textit{rhs}  \gets \Call{post-order}{\textit{node.children.second}}$
			\State $\left(\textit{alignCost}, \textit{alignContext}\right) \gets \textit{lhs.prelimString} \otimes \textit{rhs.prelimString}$ \Comment{See Algorithm \ref{Alg:pairwiseAlignment}}
			\State $\textit{node.cost} \gets \textit{alignCost} + \textit{lhs.cost} + \textit{rhs.cost}$
			\State $\textit{node.prelimString} \gets \textit{alignContext}$
			\EndIf
			\EndFunction

			\Require{$inputString \in \Sigma^{*}$ }
			\Ensure{$\Sigma_{\Gamma}^{*}$ }
			\Function{initializeString}{$\textit{inputString}$}
			\State $i \gets \vert\textit{inputString} \;\vert - 1,$
			\While{$i \geq 0$}
			\State $\textit{inputString}_i \gets \textbf{ALIGN} \; \left\{ \textit{inputString}_i\right\} \left\{ \textit{inputString}_i \right\} \left\{ \textit{inputString}_i \right\}$
			\State $i \gets i - 1$
			\EndWhile
			\Return $\textit{inputString}$
			\EndFunction
			
		\end{algorithmic}
	\end{algorithm}

    \begin{algorithm}
		\caption{Pre-order Traversal}
		\label{Alg:preorder}
		\begin{algorithmic}[1]
			\Require{      A binary tree decorated with node labels $prelimString  \in \Sigma_{\Gamma}^{*}$}
			\Ensure {$\;\,$ A binary tree decorated with node labels \enskip $finalString   \in \Sigma_{\Gamma}^{*}$}
			\Function{PreOrder}{$\textit{node}$}
			\If    {isRoot ( $\textit{node}$ )} \Comment{Initialize the root node}
			\State $\textit{node.finalString} \gets \textit{node.prelimString}$
			\Else  \Comment{Derive alignment for node}
			\State $\textit{parentFinal} \;\;\; \gets \textit{node.parent.finalString}$
			\State $\textit{parentPrelim}    \, \gets \textit{node.parent.prelimString}$
			\State $\textit{childPrelim} \;\;\;\, \gets \textit{node.prelimString}$
			\State $\textit{v} \gets$ \Call{DeriveAlignment}{$isLeft ( \textit{node} ), \textit{parentFinal}, \textit{parentPrelim}, \textit{childPrelim}$}
			\State $\textit{node.finalString} \gets \textit{v}$
			\EndIf
			\State $\Call{PreOrder}{\textit{node.children.first}}$
			\State $\Call{PreOrder}{\textit{node.children.second}}$
			\EndFunction
			
		\end{algorithmic}
    \end{algorithm}

    \begin{algorithm}
		\label{Alg:pre-order}
		\caption{Non-root Node Alignment}\label{Alg:deriveAlignment}
		\begin{algorithmic}[1]
			\Require{$isLeft \in \left\{ True,\, False \right\}$}
			\Require{$\textit{pAlignment},\, \textit{pContext},\, \textit{cContext} \in \Sigma_{\Gamma}^*$}
			\Ensure {$\;\;\,\Sigma_{\Gamma}^{*}$}
			\Function{DeriveAlignment}{$\textit{isLeft},\,  \textit{pAlignment},\, \textit{pContext},\, \textit{cContext}$}
			\State $\textit{paLen} \enskip \gets$ \Call{length}{$\textit{pAlignment}$}
			\State $\textit{ccLen} \enskip\, \gets$ \Call{length}{$\textit{cContext}$}
			\State $\left( \textit{i}, \textit{j}, \textit{k} \right) \;\gets \left( 0, 0, 0 \right)$
			\For{$\textit{i} < \textit{paLen}$}
			\If {$\;\;\; \textit{pAlignment}_i = \texttt{GAPPED}$ \Comment{Case 0}\par\hskip\algorithmicindent \enskip
				$\lor \; \textit{k} \geq \textit{ccLen}$ \Comment{Case 1}\par\hskip\algorithmicindent\hspace{-0.8em}}
			\State $\textit{result}_i \gets \texttt{GAPPED}$
			\Else
			\If {$\enskip\; \textit{pAlignment}_i = \texttt{BOTH}$\Comment{Case 2}\par\hskip\algorithmicindent \quad\enskip
				$\lor \left( \textit{pAlignment}_i = \textit{pContext}_j = \texttt{LEFT} \;\;\; \land \;\;\, isLeft \right)$\Comment{Case 3}\par\hskip\algorithmicindent\quad\enskip
				$\lor \left( \textit{pAlignment}_i = \textit{pContext}_j = \texttt{RIGHT} \; \land \neg   isLeft \right)$\Comment{Case 4}\par\hskip\algorithmicindent$\;\;\;$}  
			\State $\textit{result}_i \gets \textit{cContext}_k$
			\State $\textit{k} \gets \textit{k} + 1$
			\Else \Comment{Case 5}
			\State $\textit{result}_i \gets \texttt{GAPPED}$
			\EndIf
					
			%          \Switch{$\textit{pAlignment}_i$}
			%            
			%            \Case{$\texttt{BOTH}$} \Comment{Case 1}
			%            \State $\textit{result}_i \gets \textit{cContext}_k$
			%            \State $\textit{k} \gets \textit{k} + 1$
			%            \EndCase
			%                      
			%            \Case{$\texttt{LEFT}}$
			%              \Switch{$\textit{pContext}_j$}
			%                \Case{$\texttt{LEFT}$} \Comment{Case 2}
			%                \State $\textit{result}_i \gets \texttt{GAPPED}$
			%                \EndCase
			%                \Default \Comment{Case 3}
			%                \State $\textit{result}_i \gets \textit{cContext}_k$
			%                \State $\textit{k} \gets \textit{k} + 1$
			%                \EndDefault
			%              \EndSwitch
			%            \EndCase
			%            
			%            \Case{$\texttt{RIGHT}$} 
			%              \Switch{$\textit{pContext}_j$}
			%                \Case{$\texttt{RIGHT}$} \Comment{Case 4}
			%                \State $\textit{result}_i \gets \textit{cContext}_k$
			%                \State $\textit{k} \gets \textit{k} + 1$
			%                \EndCase
			%                \Default \Comment{Case 5}
			%                \State $\textit{result}_i \gets \texttt{GAPPED}$
			%                \EndDefault
			%              \EndSwitch
			%            \EndCase
			%
			%            \Case{$\texttt{GAPPED}$} \Comment{Case 6}
			%            \State $\textit{result}_i \gets \texttt{GAPPED}$
			%            \EndCase
			%
			%          \EndSwitch
			
			\State $\textit{j} \gets \textit{j} + 1$
			\EndIf
			\State $\textit{i} \gets \textit{i} + 1$
			\EndFor
			\Return $\textit{result}$
			\EndFunction
		\end{algorithmic}
	\end{algorithm}

\end{backmatter}

\end{document}
